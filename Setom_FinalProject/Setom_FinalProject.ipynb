{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETOM_501A Final Project\n",
    "\n",
    "This final project will be part of the MSGT capstone project. The project accepts shapefiles or folders of shapefiles, runs them against a GIS model, and then generates a single output shapefile. This final project will perform these acts at a high level in order to prove the concept. In particular it will search for new files in a staging folder and process them against a simple data model, outputting a single file. The exact procedure is described in the code comments. A high-level description is available here: \n",
    "\n",
    "### Data Pipeline\n",
    "The data passes through multiple stages as it is processed. This is most easily described as three phases: \n",
    " - Data is ingested, placed in a named folder. That folder is placed in a stage folder\n",
    " - The names of each folder are checked against existing intermediary folders. If an intermediary folder exists, the new data must be an addition to that dataset, if no folder exists then the data is a new dataset. Add new data or create new dataset\n",
    " - For each modified or new dataset, process the new files\n",
    " - Remove the new files from the dataset. The end result of each dataset folder should be a single file\n",
    " - Merge all singular dataset files into a final, single file output\n",
    "\n",
    "The end result is: MANY input files --> SOME dataset files --> ONE output file\n",
    "\n",
    "### Project Fundamental Pseudocode\n",
    " - Scan a folder for files\n",
    " - If new files, check for the existence of a dataset \n",
    " - If existing dataset, add files to that dataset, Update folder metadata file to flag processing required\n",
    " - If no existing dataset, create new dataset, add files to dataset, flag dataset for processing, clear Stage folder,  Create a folder metadata file including: date created, date modified, Flag for modification required\n",
    " - Process all flagged datasets, discard files as they are processed (end result should be one file per dataset)\n",
    " - Merge all datasets\n",
    " - Output single file, overwrite existing output\n",
    " \n",
    " #### Development Notes\n",
    " I had initially planned to check for updates using folder timestamps. However I think this is a bad design. If a folder is somehow missed, the chron job will not pick it up next time around, since the modified date will remain the same. For this reason I'm going with a flag/metadata file for each folder. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ...\n",
      "Stage Files Imported...Processing...\n",
      "Job complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "\n",
    "#Define paths to different folders\n",
    "stagePath = \"Stage/\"\n",
    "datasetPath = \"Datasets/\"\n",
    "outputPath = \"Output/\"\n",
    "#Define a list of directories that need to be processed\n",
    "toProcess = []\n",
    "\n",
    "#Checks for the existence of any files in a given path NOTE: Empty directoris within the path will return FALSE (Files only!)\n",
    "# IN: A path to check\n",
    "# OUT: True if files, False if no files\n",
    "def checkForNewFiles(path) : \n",
    "    if len(os.listdir(path)) > 0 :\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#Checks for the existence of any datasets with the given name\n",
    "#IN: (The name of the new dataset), (The path of the directory to check)\n",
    "#OUT: True if exists\n",
    "def checkForExistingDataset(name, path) :\n",
    "    for directory in os.listdir(path) :\n",
    "        if not directory.startswith('.') :\n",
    "            if directory == name :\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "#Moves a file from a source directory to a destination directory - Retains the filename\n",
    "#IN: (Source Directory), (Destination Directory), (Filename)\n",
    "#OUT: Nil\n",
    "def copyFile(source, dest, fileName) :\n",
    "    src = source+fileName\n",
    "    dst = dest+fileName\n",
    "    shutil.copyfile(src,dst)\n",
    "\n",
    "#Process Contents of Stage into Datasets - Takes staged folders, \n",
    "#puts them in apropriate Dataset folders. Updates Global toProcess list. Cleans Stage\n",
    "#IN: Nothing\n",
    "#OUT: Returns True if something happened\n",
    "def processStage() :\n",
    "    flag = False\n",
    "    if(checkForNewFiles(stagePath)) :\n",
    "        #For each new dataset: \n",
    "        for directory in os.listdir(stagePath) :\n",
    "            if not directory.startswith('.') :\n",
    "                flag = True\n",
    "                #If the dataset already exists add the new data to it: \n",
    "                if(checkForExistingDataset(directory, datasetPath)) :\n",
    "                    pass\n",
    "                #Else it does not exist, so make a new dataset\n",
    "                else : \n",
    "                    pathlib.Path(datasetPath+directory).mkdir() \n",
    "                #Now copy files to the datasets\n",
    "                files = os.listdir(stagePath+directory)\n",
    "                filesCopied = 0\n",
    "                for f in files :\n",
    "                    copyFile(stagePath+directory+\"/\", datasetPath+directory+\"/\", f)\n",
    "                    filesCopied += 1\n",
    "                #print(\"%s files copied to dataset: %s\" % (filesCopied, directory))\n",
    "                #update Global toProcessList\n",
    "                global toProcess\n",
    "                toProcess.append(directory)\n",
    "                #Finally, delete the Directory from Stage\n",
    "                shutil.rmtree(stagePath+directory)\n",
    "        if flag :\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "#Process a dataset, given a path, take all the shapefiles in that directory and process against a model\n",
    "#Checks for existence of {dataset}.shp, if it exists, uses it. If not exists, creates it\n",
    "#IN: Path to the directory you wish to process, name of the dataset\n",
    "#OUT: Nil\n",
    "def processDataset(path) :\n",
    "    #Check for the dataset master, make if if there isn't one\n",
    "    dataset = datasetPath + path + \"/\" + path + \".txt\"\n",
    "    if not (os.path.isfile(dataset)) :\n",
    "        with open(dataset, \"w+\"): pass\n",
    "    #run the model\n",
    "\n",
    "    #delete the superfluous files\n",
    "    for file in os.listdir(datasetPath+path) :\n",
    "        if (str(file) == path+\".txt\") :\n",
    "            pass\n",
    "        else :\n",
    "            os.remove(datasetPath + path + \"/\" + file) \n",
    "    \n",
    "\n",
    "\n",
    "#-----*********MAIN*********-----\n",
    "print(\"Running ...\")\n",
    "if(processStage()) :\n",
    "    print(\"Stage Files Imported...Processing...\")\n",
    "    for directory in toProcess: \n",
    "        processDataset(directory)\n",
    "    #reset toProcess\n",
    "    toProcess = []\n",
    "\n",
    "print(\"Job complete!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
